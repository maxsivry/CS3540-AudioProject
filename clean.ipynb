{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "(For convience, the cleaned dataset has been included as dataset.zip)\n",
    "\n",
    "The purpose of this notebook is to clean our original datasource (https://huggingface.co/datasets/amaai-lab/MusicBench). Our full dataset was over 24GB, which presented a massive problem when trying to transmit the data, store it (too large for GitHub, Drive, etc.), or work with it. Operations on the data each took over an hour. Because of this, we have created this notebook which augments the original dataset, reducing its size (we were going to reduce the size in our original data cleaning pipeline anyway, we are just doing it up front to reduce the storage size). Thus, this notebook should be run ONCE after downloading the data from its source. The original dataset must be in the following directory format:\n",
    "\n",
    "\\dataset\\\n",
    "&ensp;&ensp;&ensp;\\data\\\n",
    "&ensp;&ensp;&ensp;\\data_aug2\\\n",
    "&ensp;&ensp;&ensp;MusicBench_train.json\n",
    "\n",
    "After running the following code, the original dataset will have been replaced by the dataset used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WAV files not found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "df = pd.read_json(\"dataset/MusicBench_train.json\", lines=True)\n",
    "df = df.drop_duplicates(subset=[\"location\"])\n",
    "# Remove irrelavent features\n",
    "df = df[[\"location\", \"key\", \"keyprob\"]]\n",
    "df[\"keyprob\"] = df[\"keyprob\"].map(lambda x: x[0])\n",
    "# Remove instances below the probability threshold\n",
    "df_keep = df[df[\"keyprob\"] > THRESHOLD]\n",
    "df_trash = df[df[\"keyprob\"] <= THRESHOLD]\n",
    "# Delete all of the files associated with the removed instances\n",
    "locations = df_trash[\"location\"].to_numpy()\n",
    "not_found_count = 0\n",
    "for loc in locations:\n",
    "    try:\n",
    "        pathlib.Path(\"dataset/\" + loc).unlink()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Not Found: dataset/\" + loc)\n",
    "        not_found_count += 1\n",
    "print(str(not_found_count) + \" WAV files not found\")\n",
    "# Overwrite the old json metadata file\n",
    "df_keep = df_keep[[\"location\", \"key\"]]\n",
    "df_keep.to_json(\"dataset/metadata.json\", orient=\"records\", lines=True)\n",
    "pathlib.Path(\"dataset/MusicBench_train.json\").unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# A double-check that all files which are referenced in the metadata exist\n",
    "df2 = pd.read_json(\"dataset/metadata.json\", lines=True)\n",
    "not_found = 0\n",
    "locations = df2[\"location\"].to_numpy()\n",
    "for loc in locations:\n",
    "    if not pathlib.Path(\"dataset/\" + loc).exists:\n",
    "        not_found += 1\n",
    "print(not_found)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
